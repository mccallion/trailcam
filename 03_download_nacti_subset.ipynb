{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8f25c15",
   "metadata": {},
   "source": [
    "# Downloading North American Camera Trap Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f16b036",
   "metadata": {},
   "source": [
    "https://lila.science/datasets/nacti\n",
    "\n",
    "\"This data set contains 3.7M camera trap images from five locations across the United States, with labels for 28 animal categories, primarily at the species level (for example, the most common labels are cattle, boar, and red deer). Approximately 12% of images are labeled as empty. We have also added bounding box annotations to 8892 images (mostly vehicles and birds).\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8638a1c9",
   "metadata": {},
   "source": [
    "Goal: Download max(all,55K) images for all classes in the NACTI dataset (incl. empty). I have 500GB to work with.\n",
    "\n",
    "The dataset maintainers provide these instructions for downloading their data:\n",
    "- [lila database \"sas urls\"](https://lila.science/wp-content/uploads/2020/03/lila_sas_urls.txt)\n",
    "- [example python script](https://github.com/microsoft/CameraTraps/blob/master/data_management/download_lila_subset.py)\n",
    "- [quick writeup on different ways to download the data](https://lila.science/image-access)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab10de5",
   "metadata": {},
   "source": [
    "# Specifying our subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f7857b",
   "metadata": {},
   "source": [
    "We're going to download the images into our local data directory and into a new folder named `nacti`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad2c9001",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T01:44:26.280642Z",
     "start_time": "2021-12-29T01:44:24.649483Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import requests\n",
    "\n",
    "DATA = Path(\"/home/rory/data\")\n",
    "NACTI = DATA / \"nacti\"\n",
    "ZIP = NACTI / \"metadata.json.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964b1b03",
   "metadata": {},
   "source": [
    "We're going to first download the NACTI annotations file (named `metadata.json`). We need to do this so we can find out exactly what the animal classes are named. We need exact names because the script to download subsets requires those exact names. (Note that in the output below, you should only have the `metadata.json.zip` file if you're running this for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0f552aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T01:44:30.547296Z",
     "start_time": "2021-12-29T01:44:26.281535Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Path('/home/rory/data/nacti/models'),\n",
       " Path('/home/rory/data/nacti/backups'),\n",
       " Path('/home/rory/data/nacti/imgs'),\n",
       " Path('/home/rory/data/nacti/bad_imgs'),\n",
       " Path('/home/rory/data/nacti/bad_paths-empty.txt'),\n",
       " Path('/home/rory/data/nacti/metadata.json'),\n",
       " Path('/home/rory/data/nacti/metadata.json.zip'),\n",
       " Path('/home/rory/data/nacti/lila_sas_urls.txt'),\n",
       " Path('/home/rory/data/nacti/bad_img_files.txt'),\n",
       " Path('/home/rory/data/nacti/urls_to_download-empty.txt'),\n",
       " Path('/home/rory/data/nacti/2021-12-13-1830_cats6_err036')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL = 'https://lilablobssc.blob.core.windows.net/nacti/nacti_metadata.json.zip'\n",
    "\n",
    "r = requests.get(URL)\n",
    "with open(ZIP, 'wb') as f:\n",
    "    f.write(r.content)\n",
    "\n",
    "list(NACTI.ls())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13580ea3",
   "metadata": {},
   "source": [
    "I then ran `unzip` in my terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53de4bcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T01:44:30.552860Z",
     "start_time": "2021-12-29T01:44:30.550755Z"
    }
   },
   "outputs": [],
   "source": [
    "#!unzip ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32061bec",
   "metadata": {},
   "source": [
    "Let's take a look at the annos json's highest level keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17ca3649",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T01:44:38.382165Z",
     "start_time": "2021-12-29T01:44:30.553643Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59, dict_keys(['images', 'info', 'categories', 'annotations']))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annos = load_json(NACTI_ANNOS)\n",
    "len(annos['categories']) , annos.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e961cf79",
   "metadata": {},
   "source": [
    "We can tell already that this probably follows the COCO json annos format. Let's take a look at one of the records in `categories`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4f25ffa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T01:44:38.386154Z",
     "start_time": "2021-12-29T01:44:38.383183Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'name': 'alces alces',\n",
       " 'species': 'alces alces',\n",
       " 'genus': 'alces',\n",
       " 'family': 'cervidae',\n",
       " 'ord': 'artiodactyla',\n",
       " 'class': 'mammalia',\n",
       " 'common name': 'moose'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annos['categories'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21951e0a",
   "metadata": {},
   "source": [
    "After looking at this record, it's clear that I personally need to work with the `common name` field because I don't know the \"binomial\" name of virtually any animal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "791e1321",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T01:44:38.394616Z",
     "start_time": "2021-12-29T01:44:38.387097Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['raccoon',\n",
       " 'vehicle',\n",
       " 'california quail',\n",
       " 'american marten',\n",
       " 'unidentified bird',\n",
       " 'american red squirrel',\n",
       " 'ermine',\n",
       " 'wolf',\n",
       " 'black-tailed jackrabbit',\n",
       " 'gray fox',\n",
       " 'cougar',\n",
       " 'fox squirrel',\n",
       " 'domestic cow',\n",
       " 'horse',\n",
       " 'eastern gray squirrel',\n",
       " 'unidentified pocket gopher',\n",
       " 'empty',\n",
       " 'elk',\n",
       " 'red deer',\n",
       " 'black rat',\n",
       " 'white-tailed deer',\n",
       " 'bobcat',\n",
       " 'nine-banded armadillo',\n",
       " 'striped skunk',\n",
       " 'domestic dog',\n",
       " 'north american porcupine',\n",
       " 'california ground squirrel',\n",
       " 'dark-eyed junco',\n",
       " 'virginia opossum',\n",
       " 'unidentified pack rat',\n",
       " 'yellow-bellied marmot',\n",
       " 'wild turkey',\n",
       " 'unidentified sciurus',\n",
       " 'unidentified deer',\n",
       " 'house wren',\n",
       " 'coyote',\n",
       " 'long-tailed weasel',\n",
       " 'moose',\n",
       " 'unidentified corvus',\n",
       " 'mule deer',\n",
       " 'unidentified mouse',\n",
       " \"steller's jay\",\n",
       " 'wild boar',\n",
       " 'american crow',\n",
       " 'mourning dove',\n",
       " 'donkey',\n",
       " 'unidentified chipmunk',\n",
       " 'unidentified rodent',\n",
       " 'american black bear',\n",
       " 'gray jay',\n",
       " 'unidentified deer mouse',\n",
       " 'unidentified rabbit',\n",
       " 'north american river otter',\n",
       " 'european badger',\n",
       " 'dusky grouse',\n",
       " 'snowshoe hare',\n",
       " 'unidentified accipitrid',\n",
       " 'eastern cottontail',\n",
       " 'red fox']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_names = list(set([a['common name'] for a in annos['categories']]))\n",
    "common_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da581159",
   "metadata": {},
   "source": [
    "I need to pass the subset-downloader script a list of the names of classes. Each name in that list needs to be the `name` field (not the `common_name` field). It will then download all of the images in that class. (Note that 'person' is not a category due to the legal PII issues it would create. Don't share images of people without their consent!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8136a76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T01:49:26.987756Z",
     "start_time": "2021-12-29T01:49:26.985304Z"
    }
   },
   "outputs": [],
   "source": [
    "my_common_names = [\n",
    "    'california quail',\n",
    "    'wild turkey',\n",
    "    'mule deer',\n",
    "    'cougar',\n",
    "    'california ground squirrel',\n",
    "    'american black bear',\n",
    "    'vehicle',\n",
    "    'empty'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5af5f71",
   "metadata": {},
   "source": [
    "Now I just map the `common_name` back to the `name` to get my final list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73a0e92b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T01:49:28.323061Z",
     "start_time": "2021-12-29T01:49:28.319318Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['callipepla californica',\n",
       " 'meleagris gallopavo',\n",
       " 'odocoileus hemionus',\n",
       " 'puma concolor',\n",
       " 'otospermophilus beecheyi',\n",
       " 'ursus americanus',\n",
       " 'vehicle',\n",
       " 'empty']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_names = [a['name'] for a in annos['categories'] if a['common name'] in my_common_names]\n",
    "my_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0003553d",
   "metadata": {},
   "source": [
    "# Downloading our subset with `download_lila_subset.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed3cc09",
   "metadata": {},
   "source": [
    "The following code is a very slightly altered version of the script provided by the Microsoft camera trap team that's available from their GitHub [here](https://github.com/microsoft/CameraTraps/blob/master/data_management/download_lila_subset.py). A lot of the code pertains to using `azcopy` (which I didn't use)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9574deb8",
   "metadata": {},
   "source": [
    "## Get links to the images of animals in our subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93598fa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T01:49:41.554129Z",
     "start_time": "2021-12-29T01:49:30.961512Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# download_lila_subset.py\n",
    "#\n",
    "# Example of how to download a list of files from LILA, e.g. all the files\n",
    "# in a data set corresponding to a particular species.\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "# ----- Imports ----- #\n",
    "\n",
    "import json\n",
    "import urllib.request\n",
    "import tempfile\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----- Constants ----- #\n",
    "\n",
    "# This file specifies which datasets are available\n",
    "metadata_url = 'http://lila.science/wp-content/uploads/2020/03/lila_sas_urls.txt'\n",
    "\n",
    "# List of the datasets we want images from\n",
    "datasets_of_interest = ['NACTI']\n",
    "\n",
    "# Our subset of species\n",
    "species_of_interest = my_names\n",
    "\n",
    "# Where we'll save the downloaded images\n",
    "output_dir = \"/home/rory/data/nacti/downloads\"\n",
    "os.makedirs(output_dir,exist_ok=True)\n",
    "overwrite_files = False\n",
    "n_download_threads = 50\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----- Helper Functions ----- #\n",
    "\n",
    "def download_url(url, destination_filename=None, force_download=False, verbose=True):\n",
    "    \"\"\"\n",
    "    Download a URL (defaulting to a temporary file)\n",
    "    \"\"\"\n",
    "    if destination_filename is None:\n",
    "        temp_dir = os.path.join(tempfile.gettempdir(),'lila')\n",
    "        os.makedirs(temp_dir,exist_ok=True)\n",
    "        url_as_filename = url.replace('://', '_').replace('.', '_').replace('/', '_')\n",
    "        destination_filename = \\\n",
    "            os.path.join(temp_dir,url_as_filename)\n",
    "            \n",
    "    if (not force_download) and (os.path.isfile(destination_filename)):\n",
    "        print('Bypassing download of already-downloaded file {}'.format(os.path.basename(url)))\n",
    "        return destination_filename\n",
    "    \n",
    "    if verbose:\n",
    "        print('Downloading file {} to {}'.format(os.path.basename(url),destination_filename),end='')\n",
    "    \n",
    "    os.makedirs(os.path.dirname(destination_filename),exist_ok=True)\n",
    "    urllib.request.urlretrieve(url, destination_filename)  \n",
    "    assert(os.path.isfile(destination_filename))\n",
    "    \n",
    "    if verbose:\n",
    "        nBytes = os.path.getsize(destination_filename)    \n",
    "        print('...done, {} bytes.'.format(nBytes))\n",
    "        \n",
    "    return destination_filename\n",
    "\n",
    "\n",
    "def download_relative_filename(url, output_base, verbose=False):\n",
    "    \"\"\"\n",
    "    Download a URL to output_base, preserving relative path\n",
    "    \"\"\"\n",
    "    p = urlparse(url)\n",
    "    # remove the leading '/'\n",
    "    assert p.path.startswith('/'); relative_filename = p.path[1:]\n",
    "    destination_filename = os.path.join(output_base,relative_filename)\n",
    "    download_url(url, destination_filename, verbose=verbose)\n",
    "    \n",
    "    \n",
    "def unzip_file(input_file, output_folder=None):\n",
    "    \"\"\"\n",
    "    Unzip a zipfile to the specified output folder, defaulting to the same location as\n",
    "    the input file    \n",
    "    \"\"\"\n",
    "    if output_folder is None:\n",
    "        output_folder = os.path.dirname(input_file)\n",
    "        \n",
    "    with zipfile.ZipFile(input_file, 'r') as zf:\n",
    "        zf.extractall(output_folder)\n",
    "\n",
    "        \n",
    "        \n",
    "              \n",
    "# ----- First, download and parse the metadata file ----- #\n",
    "\n",
    "# Put the master metadata file in the same folder where we're putting images\n",
    "p = urlparse(metadata_url)\n",
    "metadata_filename = os.path.join(output_dir,os.path.basename(p.path))\n",
    "download_url(metadata_url, metadata_filename)\n",
    "\n",
    "# Read lines from the master metadata file\n",
    "with open(metadata_filename,'r') as f:\n",
    "    metadata_lines = f.readlines()\n",
    "metadata_lines = [s.strip() for s in metadata_lines]\n",
    "\n",
    "# Parse those lines into a table\n",
    "metadata_table = {}\n",
    "\n",
    "for s in metadata_lines:\n",
    "    \n",
    "    if len(s) == 0 or s[0] == '#':\n",
    "        continue\n",
    "    \n",
    "    # Each line in this file is name/sas_url/json_url\n",
    "    tokens = s.split(',')\n",
    "    assert len(tokens)==3\n",
    "    url_mapping = {'sas_url':tokens[1],'json_url':tokens[2]}\n",
    "    metadata_table[tokens[0]] = url_mapping\n",
    "    \n",
    "    assert 'https' not in tokens[0]\n",
    "    assert 'https' in url_mapping['sas_url']\n",
    "    assert 'https' in url_mapping['json_url']\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# ----- Second, download and extract metadata for the specified datasets ----- #\n",
    "\n",
    "for ds_name in datasets_of_interest:\n",
    "    \n",
    "    assert ds_name in metadata_table\n",
    "    json_url = metadata_table[ds_name]['json_url']\n",
    "    \n",
    "    p = urlparse(json_url)\n",
    "    json_filename = os.path.join(output_dir,os.path.basename(p.path))\n",
    "    download_url(json_url, json_filename)\n",
    "    \n",
    "    # Unzip if necessary\n",
    "    if json_filename.endswith('.zip'):\n",
    "        \n",
    "        with zipfile.ZipFile(json_filename,'r') as z:\n",
    "            files = z.namelist()\n",
    "        assert len(files) == 1\n",
    "        unzipped_json_filename = os.path.join(output_dir,files[0])\n",
    "        if not os.path.isfile(unzipped_json_filename):\n",
    "            unzip_file(json_filename,output_dir)        \n",
    "        else:\n",
    "            print('{} already unzipped'.format(unzipped_json_filename))\n",
    "        json_filename = unzipped_json_filename\n",
    "    \n",
    "    metadata_table[ds_name]['json_filename'] = json_filename\n",
    "    # ...for each dataset of interest\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----- Third, make the list of files to download (for all data sets) ----- #\n",
    "\n",
    "# Flat list or URLS, for use with direct Python downloads\n",
    "urls_to_download = []\n",
    "\n",
    "# For use with azcopy\n",
    "downloads_by_dataset = {}\n",
    "\n",
    "for ds_name in datasets_of_interest:\n",
    "    \n",
    "    json_filename = metadata_table[ds_name]['json_filename']\n",
    "    sas_url = metadata_table[ds_name]['sas_url']\n",
    "    \n",
    "    base_url = sas_url.split('?')[0]    \n",
    "    assert not base_url.endswith('/')\n",
    "    \n",
    "    sas_token = sas_url.split('?')[1]\n",
    "    assert not sas_token.startswith('?')\n",
    "    \n",
    "    ## Open the metadata file\n",
    "    \n",
    "    with open(json_filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    categories = data['categories']\n",
    "    for c in categories:\n",
    "        c['name'] = c['name'].lower()\n",
    "    category_id_to_name = {c['id']:c['name'] for c in categories}\n",
    "    annotations = data['annotations']\n",
    "    images = data['images']\n",
    "\n",
    "\n",
    "    ## Build a list of image files (relative path names) that match the target species\n",
    "\n",
    "    category_ids = []\n",
    "    \n",
    "    for species_name in species_of_interest:\n",
    "        matching_categories = list(filter(lambda x: x['name'] == species_name, categories))\n",
    "        if len(matching_categories) == 0:\n",
    "            continue\n",
    "        assert len(matching_categories) == 1\n",
    "        category = matching_categories[0]\n",
    "        category_id = category['id']\n",
    "        category_ids.append(category_id)\n",
    "    \n",
    "    print('Found {} matching categories for data set {}:'.format(len(category_ids),ds_name))\n",
    "    \n",
    "    if len(category_ids) == 0:\n",
    "        continue\n",
    "    \n",
    "    for i_category,category_id in enumerate(category_ids):\n",
    "        print(category_id_to_name[category_id],end='')\n",
    "        if i_category != len(category_ids) -1:\n",
    "            print(',',end='')\n",
    "    print('')\n",
    "    \n",
    "    # Retrieve all the images that match that category\n",
    "    image_ids_of_interest = set([ann['image_id'] for ann in annotations if ann['category_id'] in category_ids])\n",
    "    \n",
    "    print('Selected {} of {} images for dataset {}'.format(len(image_ids_of_interest),len(images),ds_name))\n",
    "    \n",
    "    # Retrieve image file names\n",
    "    filenames = [im['file_name'] for im in images if im['id'] in image_ids_of_interest]\n",
    "    assert len(filenames) == len(image_ids_of_interest)\n",
    "    \n",
    "    # Convert to URLs\n",
    "    for fn in filenames:        \n",
    "        url = base_url + '/' + fn\n",
    "        urls_to_download.append(url)\n",
    "\n",
    "    downloads_by_dataset[ds_name] = {'sas_url':sas_url,'filenames':filenames}\n",
    "    \n",
    "# ...for each dataset\n",
    "\n",
    "\n",
    "\n",
    "print('Found {} images to download'.format(len(urls_to_download)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583e7dcd",
   "metadata": {},
   "source": [
    "Note that there are many `empty` images, and I didn't use all of them due to limited disk space. To save space, I ran this script twice: once to get the animals and vehicles, and a second time to get the `empty` images. On that second time, I only kept a random subset of the links to empty images by running the following line of code at this step (I skipped this step the first go-around):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42978227",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T01:51:50.243340Z",
     "start_time": "2021-12-29T01:51:49.936250Z"
    }
   },
   "outputs": [],
   "source": [
    "urls_to_download = L(urls_to_download).shuffle()[:55_000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf001e72",
   "metadata": {},
   "source": [
    "## Download images from the list of links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e09d9707",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-13T22:34:46.291890Z",
     "start_time": "2021-12-13T22:34:45.992109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 55000 files to /home/rory/data/nacti/downloads ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f8bc8c294c4f49a5170a7e78dafae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished downloading files.\n"
     ]
    }
   ],
   "source": [
    "# ----- Download images ----- #\n",
    "\n",
    "def download_file(url, output_base, verbose=False):\n",
    "    \"\"\"\n",
    "    Download a URL to output_base, preserving relative path\n",
    "    \"\"\"\n",
    "    p = urlparse(url)\n",
    "    # remove the leading '/'\n",
    "    assert p.path.startswith('/'); relative_filename = p.path[1:]\n",
    "    destination_filename = os.path.join(output_base,relative_filename)\n",
    "    download_url(url, destination_filename, verbose=verbose)\n",
    "\n",
    "    \n",
    "def download_from_list(urls, dest, n_threads=50):\n",
    "    n = len(urls)\n",
    "    print(f\"Downloading {n} files to {dest} ...\")\n",
    "    if n_threads <= 1:\n",
    "        for url in tqdm(urls):        \n",
    "            download_file(url, dest, verbose=True)\n",
    "    else:\n",
    "        pool = ThreadPool(n_threads)        \n",
    "        tqdm(pool.imap(lambda fn: download_file(fn, dest, verbose=False), urls), total=n)\n",
    "    print(f\"Finished downloading files.\")\n",
    "    \n",
    "    \n",
    "download_from_list(urls_to_download, output_dir, n_download_threads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Headings",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
